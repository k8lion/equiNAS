{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'models' from '/Users/kaitlinmaile/repos/equiNAS/models.py'>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import utilities\n",
    "import models\n",
    "import torch\n",
    "import importlib\n",
    "importlib.reload(utilities)\n",
    "importlib.reload(models)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.TDRegEquiCNN()\n",
    "train_loader, validation_loader, test_loader = utilities.get_mnist_dataloaders(\"/home/kaitlin/repos\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import Module\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class LeNet5(Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(256, 120)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.relu4 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        self.relu5 = nn.ReLU()\n",
    "        self.uuid = \"null\"\n",
    "        self.gs = [(0,0) for _ in range(6)]\n",
    "        self.loss_function = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = self.conv1(x)\n",
    "        y = self.relu1(y)\n",
    "        y = self.pool1(y)\n",
    "        y = self.conv2(y)\n",
    "        y = self.relu2(y)\n",
    "        y = self.pool2(y)\n",
    "        y = y.view(y.shape[0], -1)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu3(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.relu4(y)\n",
    "        y = self.fc3(y)\n",
    "        y = self.relu5(y)\n",
    "        return y\n",
    "\n",
    "model = LeNet5() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import RandomRotation, Pad, Resize, ToTensor, Compose\n",
    "from torchvision.transforms.functional import InterpolationMode\n",
    "from torchvision import datasets\n",
    "\n",
    "class MnistDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, mode, transform=None, path_to_dir='~'):\n",
    "        assert mode in ['train', 'test']\n",
    "            \n",
    "        if mode == \"train\":\n",
    "            file = path_to_dir+\"/data/mnist_train.amat\"\n",
    "        else:\n",
    "            file = path_to_dir+\"/data/mnist_test.amat\"\n",
    "        \n",
    "        self.transform = transform\n",
    "        print(file)\n",
    "        data = np.genfromtxt(file, delimiter='   ')\n",
    "        print(np.shape(data))\n",
    "            \n",
    "        self.images = data[:, :-1].reshape(-1, 28, 28).astype(np.float32)\n",
    "        self.labels = data[:, -1].astype(np.int64)\n",
    "        self.num_samples = len(self.labels)\n",
    "\n",
    "        #self.images = np.pad(self.images, pad_width=((0,0), (2, 3), (2, 3)), mode='edge')\n",
    "\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image, label = self.images[index], self.labels[index]\n",
    "        image = Image.fromarray(image, mode='F')\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        return image, label\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "\n",
    "def get_vanilla_dataloaders(path_to_dir = \"~\"):\n",
    "    pad = Pad((0, 0, 1, 1), fill=0)\n",
    "    resize1 = Resize(87)\n",
    "    resize2 = Resize(29)\n",
    "    totensor = ToTensor()\n",
    "    train_transform = Compose([\n",
    "        pad,\n",
    "        resize1,\n",
    "        RandomRotation(180., interpolation=InterpolationMode.BILINEAR, expand=False),\n",
    "        resize2,\n",
    "        totensor,\n",
    "    ])\n",
    "    test_transform = Compose([\n",
    "        pad,\n",
    "        totensor,\n",
    "    ])\n",
    "\n",
    "    mnist_train = MnistDataset(mode='train', transform=test_transform, path_to_dir=path_to_dir)\n",
    "\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42\n",
    "    validation_split = .2\n",
    "    dataset_size = mnist_train.num_samples\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    #train_loader = torch.utils.data.DataLoader(mnist_train, batch_size=64)\n",
    "    train_loader = DataLoader(mnist_train, batch_size=64, \n",
    "                                            sampler=train_sampler)\n",
    "    validation_loader = DataLoader(mnist_train, batch_size=64,\n",
    "                                                    sampler=valid_sampler)\n",
    "\n",
    "    \n",
    "    mnist_test = MnistDataset(mode='test', transform=test_transform, path_to_dir=path_to_dir)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=64)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader\n",
    "\n",
    "def get_generic_dataloaders(path_to_dir = \"~\"):\n",
    "    pad = Pad((0, 0, 1, 1), fill=0)\n",
    "    resize1 = Resize(87)\n",
    "    resize2 = Resize(29)\n",
    "    totensor = ToTensor()\n",
    "    train_transform = Compose([\n",
    "        pad,\n",
    "        resize1,\n",
    "        RandomRotation(180., interpolation=InterpolationMode.BILINEAR, expand=False),\n",
    "        resize2,\n",
    "        totensor,\n",
    "    ])\n",
    "    test_transform = Compose([\n",
    "        pad,\n",
    "        totensor,\n",
    "    ])\n",
    "    mnist_train = datasets.MNIST(root='/home/kaitlin/repos/data/mnist_data', \n",
    "                                        train=True, \n",
    "                                        transform=test_transform,\n",
    "                                        download=True)\n",
    "\n",
    "    shuffle_dataset = True\n",
    "    random_seed= 42\n",
    "    validation_split = .2\n",
    "    dataset_size = len(mnist_train.targets)\n",
    "    indices = list(range(dataset_size))\n",
    "    split = int(np.floor(validation_split * dataset_size))\n",
    "    if shuffle_dataset :\n",
    "        np.random.seed(random_seed)\n",
    "        np.random.shuffle(indices)\n",
    "    train_indices, val_indices = indices[split:], indices[:split]\n",
    "\n",
    "    # Creating PT data samplers and loaders:\n",
    "    train_sampler = SubsetRandomSampler(train_indices)\n",
    "    valid_sampler = SubsetRandomSampler(val_indices)\n",
    "\n",
    "    train_loader = DataLoader(mnist_train, batch_size=64, \n",
    "                                            sampler=train_sampler)\n",
    "    validation_loader = DataLoader(mnist_train, batch_size=64,\n",
    "                                                    sampler=valid_sampler)\n",
    "\n",
    "    \n",
    "    mnist_test = datasets.MNIST(root='/home/kaitlin/repos/data/mnist_data', \n",
    "                                        train=False, \n",
    "                                        transform=test_transform,\n",
    "                                        download=True)\n",
    "    test_loader = DataLoader(mnist_test, batch_size=64)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000004?line=35'>36</a>\u001b[0m inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000004?line=36'>37</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000004?line=37'>38</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000004?line=39'>40</a>\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mloss_function(outputs, labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000004?line=41'>42</a>\u001b[0m \u001b[39mif\u001b[39;00m phase \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m~/repos/equiNAS/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/equiNAS/models.py:523\u001b[0m, in \u001b[0;36mTDRegEquiCNN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    521\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x: torch\u001b[39m.\u001b[39mTensor):\n\u001b[1;32m    522\u001b[0m     \u001b[39mfor\u001b[39;00m block \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks:\n\u001b[0;32m--> 523\u001b[0m         x \u001b[39m=\u001b[39m block(x)\n\u001b[1;32m    524\u001b[0m     x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull1(x\u001b[39m.\u001b[39mreshape(x\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m], \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m))\n\u001b[1;32m    525\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfull2(x)\n",
      "File \u001b[0;32m~/repos/equiNAS/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/equiNAS/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:141\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    140\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 141\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    142\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m~/repos/equiNAS/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1111\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/repos/equiNAS/venv/lib/python3.10/site-packages/torch/nn/modules/batchnorm.py:168\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    161\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    163\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m    169\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[1;32m    170\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[1;32m    171\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[1;32m    172\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[1;32m    173\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[1;32m    175\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m    176\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[1;32m    177\u001b[0m     bn_training,\n\u001b[1;32m    178\u001b[0m     exponential_average_factor,\n\u001b[1;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[1;32m    180\u001b[0m )\n",
      "File \u001b[0;32m~/repos/equiNAS/venv/lib/python3.10/site-packages/torch/nn/functional.py:2421\u001b[0m, in \u001b[0;36mbatch_norm\u001b[0;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[1;32m   2418\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[1;32m   2419\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[0;32m-> 2421\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[1;32m   2422\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[1;32m   2423\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for lr in [1e-1, 3e-2, 1e-2, 3e-3, 1e-3, 3e-4]:\n",
    "    model = models.TDRegEquiCNN()\n",
    "    train_loader, validation_loader, test_loader = utilities.get_mnist_dataloaders(\"/Users/kaitlinmaile\")\n",
    "    device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "    model = model.to(device)\n",
    "    model.optimizer = torch.optim.SGD(model.parameters(), lr=lr)\n",
    "    dataloaders = {\n",
    "        \"train\": train_loader,\n",
    "        \"validation\": validation_loader\n",
    "    }\n",
    "    history = {}\n",
    "    epochs = 5\n",
    "    history[model.uuid] = {'train': {'loss': [], \n",
    "                            'accuracy': [], \n",
    "                            'batch': [], \n",
    "                            'batchloss': []},\n",
    "                    'trainsteps': [],\n",
    "                    'validation' : {'loss': [], \n",
    "                                    'accuracy': []},\n",
    "                    'epochsteps': [],\n",
    "                    'ghistory': []}\n",
    "    history[model.uuid][\"ghistory\"].append(model.gs)\n",
    "    for epoch in range(epochs):\n",
    "        for phase in ['train', 'validation']:\n",
    "            batch = []\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            running_count = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                loss = model.loss_function(outputs, labels)\n",
    "\n",
    "                if phase == 'train':\n",
    "                    model.optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    model.optimizer.step()\n",
    "\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                running_loss += loss.detach() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "                running_count += inputs.size(0)\n",
    "                if phase == \"train\":\n",
    "                    batch.append(running_count)\n",
    "                    history[model.uuid][phase]['batchloss'].append(loss.detach().item())\n",
    "            epoch_loss = running_loss / running_count\n",
    "            epoch_acc = running_corrects.float() / running_count\n",
    "\n",
    "            history[model.uuid][phase]['loss'].append(epoch_loss.item())\n",
    "            history[model.uuid][phase]['accuracy'].append(epoch_acc.item())\n",
    "            model.score = epoch_acc.item()\n",
    "\n",
    "    print(lr, epoch_acc.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb Cell 4'\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000003?line=0'>1</a>\u001b[0m importlib\u001b[39m.\u001b[39mreload(models)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000003?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m models\u001b[39m.\u001b[39mSkipEquiCNN()\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000003?line=3'>4</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m train_loader:\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000003?line=4'>5</a>\u001b[0m     inputs_ \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mclone()\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/kaitlinmaile/repos/equiNAS/sandbox2.ipynb#ch0000003?line=5'>6</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(inputs)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loader' is not defined"
     ]
    }
   ],
   "source": [
    "importlib.reload(models)\n",
    "\n",
    "model = models.SkipEquiCNN()\n",
    "for inputs, labels in train_loader:\n",
    "    inputs_ = inputs.clone()\n",
    "    outputs = model(inputs)\n",
    "    break\n",
    "\n",
    "for tochange in range(6):\n",
    "    print(tochange)\n",
    "    parent = models.SkipEquiCNN(gs = [(0,2) for _ in range(tochange+1)]+[(0,1) for _ in range(5-tochange)], ordered = True)\n",
    "    child = parent.offspring(tochange, (0,1))\n",
    "    print(torch.allclose(parent(inputs_.clone()), child(inputs_.clone()), rtol = 1e-1, atol = 1e-1))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5f81316b144ef951a36542af280e9bdc096e781525b360b2d5d5133146c06611"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
